{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3895159",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa063090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Kwaai/IMDB_Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67df7a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822a8345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train set:\n",
      "Number of total text: 25000\n",
      "Number of unique text: 24904\n",
      "Is there any duplicate text? : True and Duplicate numbers: 96\n",
      "\n",
      "Original test set:\n",
      "Number of total text: 25000\n",
      "Number of unique text: 24801\n",
      "Is there any duplicate text? : True and Duplicate numbers: 199\n",
      "\n",
      "Total dataset:\n",
      "Number of total text: 50000\n",
      "Number of unique text: 49582\n",
      "Is there any duplicate text? : True and Duplicate numbers: 418\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# combine train + test a full dataset\n",
    "full_dataset = concatenate_datasets([ds['train'], ds['test']])  # 50,000\n",
    "\n",
    "# count all\n",
    "total = len(full_dataset)\n",
    "total_train = len(ds['train'])\n",
    "total_test = len(ds['test'])\n",
    "\n",
    "# count unique text\n",
    "unique = len(set(full_dataset['text']))\n",
    "unique_train = len(set(ds['train']['text']))\n",
    "unique_test = len(set(ds['test']['text']))\n",
    "\n",
    "# duplicate count\n",
    "du_total = total - unique\n",
    "du_train = total_train - unique_train\n",
    "du_test = total_test - unique_test\n",
    "\n",
    "print(\"Original train set:\")\n",
    "print(f\"Number of total text: {total_train}\")\n",
    "print(f\"Number of unique text: {unique_train}\")\n",
    "print(f\"Is there any duplicate text? : {total_train != unique_train} and Duplicate numbers: {du_train}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Original test set:\")\n",
    "print(f\"Number of total text: {total_test}\")\n",
    "print(f\"Number of unique text: {unique_test}\")\n",
    "print(f\"Is there any duplicate text? : {total_test != unique_test} and Duplicate numbers: {du_test}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Total dataset:\")\n",
    "print(f\"Number of total text: {total}\")\n",
    "print(f\"Number of unique text: {unique}\")\n",
    "print(f\"Is there any duplicate text? : {total != unique} and Duplicate numbers: {du_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea5d238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 34707\n",
      "Validation size: 9916\n",
      "Test size: 4959\n",
      "Total size: 49582\n"
     ]
    }
   ],
   "source": [
    "df = full_dataset.to_pandas()\n",
    "\n",
    "# unique\n",
    "df_unique = df.drop_duplicates(subset=\"text\")\n",
    "df_unique = df.drop(columns=[\"__index_level_0__\"])\n",
    "\n",
    "# transfer to dataset\n",
    "full_dataset = Dataset.from_pandas(df_unique)\n",
    "\n",
    "# split dataset\n",
    "split1 = full_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train = split1['train']\n",
    "temp = split1['test']\n",
    "\n",
    "split2 = temp.train_test_split(test_size=1/3, seed=42)\n",
    "validation = split2['train']\n",
    "test = split2['test']\n",
    "\n",
    "\n",
    "dataset = {\n",
    "    \"train\": train,\n",
    "    \"validation\": validation,\n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "print(\"Train size:\", len(dataset[\"train\"]))\n",
    "print(\"Validation size:\", len(dataset[\"validation\"]))\n",
    "print(\"Test size:\", len(dataset[\"test\"]))\n",
    "print(\"Total size:\", len(dataset[\"train\"]) + len(dataset[\"test\"]) + len(dataset[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de3a0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Anarchy and lawlessness reign supreme in the p...      1\n",
      "1  Before I begin, a \"little\" correction: IMDb st...      0\n",
      "2  You know Jason, you know Freddy, and you know ...      0\n",
      "3  Creative use of modern and mystical elements: ...      1\n",
      "4  In the trivia section for Pet Sematary, it men...      1\n",
      "5  Despite a totally misleading advertising campa...      0\n",
      "6  Well, were to start? This is by far one of the...      0\n",
      "7  What's written on the poster is: \"At birth he ...      0\n",
      "8  Many of the earlier comments are right on the ...      1\n",
      "9  i love this show. i hate when it goes to seaso...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"][:10])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c88d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c871e393b427e99b2de1700366825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/35 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb562d73bbd4fe7bcfcf71893b1d2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108de4f2d8f54cb79b970a5fb81789ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6641057"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_csv(\"imdb_train.csv\")\n",
    "dataset['validation'].to_csv(\"imdb_validation.csv\")\n",
    "dataset['test'].to_csv(\"imdb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f03ea0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ∩ Validation: 0 overlapping samples\n",
      "Train ∩ Test:       0 overlapping samples\n",
      "Validation ∩ Test:  0 overlapping samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_val_overlap': set(),\n",
       " 'train_test_overlap': set(),\n",
       " 'val_test_overlap': set()}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# check duplicate text\n",
    "def check_data_leakage(train_path, val_path, test_path, text_column=\"text\"):\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_val = pd.read_csv(val_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    train_texts = set(df_train[text_column].dropna().unique())\n",
    "    val_texts = set(df_val[text_column].dropna().unique())\n",
    "    test_texts = set(df_test[text_column].dropna().unique())\n",
    "\n",
    "    # union check\n",
    "    train_val_overlap = train_texts & val_texts\n",
    "    train_test_overlap = train_texts & test_texts\n",
    "    val_test_overlap = val_texts & test_texts\n",
    "\n",
    "    # result\n",
    "    print(f\"Train ∩ Validation: {len(train_val_overlap)} overlapping samples\")\n",
    "    print(f\"Train ∩ Test:       {len(train_test_overlap)} overlapping samples\")\n",
    "    print(f\"Validation ∩ Test:  {len(val_test_overlap)} overlapping samples\")\n",
    "\n",
    "    if train_test_overlap:\n",
    "        print(\"\\nSample duplicate text (Train ∩ Test):\")\n",
    "        for i, text in enumerate(list(train_test_overlap)[:5]):\n",
    "            print(f\"- {text[:100]}...\")  # 只显示前 80 字符\n",
    "\n",
    "    return {\n",
    "        \"train_val_overlap\": train_val_overlap,\n",
    "        \"train_test_overlap\": train_test_overlap,\n",
    "        \"val_test_overlap\": val_test_overlap\n",
    "    }\n",
    "\n",
    "check_data_leakage(\n",
    "    train_path=\"imdb_train.csv\",\n",
    "    val_path=\"imdb_validation.csv\",\n",
    "    test_path=\"imdb_test.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
